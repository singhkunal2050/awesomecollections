{
  "repo_name": "weiaicunzai_awesome-image-classification",
  "readme_content": "\n\n# Awesome - Image Classification\n\n[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\nA curated list of deep learning image classification papers and codes since 2014, Inspired by [awesome-object-detection](https://github.com/amusi/awesome-object-detection), [deep_learning_object_detection](https://github.com/hoya012/deep_learning_object_detection) and [awesome-deep-learning-papers](https://github.com/terryum/awesome-deep-learning-papers).\n\n## Background\n\nI believe image classification is a great start point before diving into other computer vision fields, espacially\nfor begginers who know nothing about deep learning. When I started to learn computer vision, I've made a lot of mistakes, I wish someone could have told me that which paper I should start with back then. There doesn't seem to have a repository to have a list of image classification papers like [deep_learning_object_detection](https://github.com/hoya012/deep_learning_object_detection) until now. Therefore, I decided to make a repository\nof a list of deep learning image classification papers and codes to help others. My personal advice for people who\nknow nothing about deep learning, try to start with vgg, then googlenet, resnet, feel free to continue reading other listed papers or switch to other fields after you are finished.\n\n**Note: I also have a repository of pytorch implementation of some of the image classification networks, you can check out [here](https://github.com/weiaicunzai/pytorch-cifar100).**\n\n## Performance Table\n\nFor simplicity reason, I only listed the best top1 and top5 accuracy on ImageNet from the papers. Note that this does not necessarily mean one network is better than another when the acc is higher, cause some networks are focused on reducing the model complexity instead of improving accuracy, or some papers only give the single crop results on ImageNet, but others give the model fusion or multicrop results.\n\n- ConvNet: name of the covolution network\n- ImageNet top1 acc: best top1 accuracy on ImageNet from the Paper\n- ImageNet top5 acc: best top5 accuracy on ImageNet from the Paper\n- Published In: which conference or journal the paper was published in.\n\n|         ConvNet            | ImageNet top1 acc | ImageNet top5 acc |   Published In     |\n|:--------------------------:|:-----------------:|:-----------------:|:------------------:|\n|           Vgg              |      76.3         |       93.2        |      ICLR2015      |\n|        GoogleNet           |       -           |       93.33       |      CVPR2015      |\n|        PReLU-nets          |       -           |       95.06       |      ICCV2015      |\n|          ResNet            |       -           |       96.43       |      CVPR2015      |\n|       PreActResNet         |      79.9         |       95.2        |      CVPR2016      |\n|       Inceptionv3          |      82.8         |       96.42       |      CVPR2016      |\n|       Inceptionv4          |      82.3         |       96.2        |      AAAI2016      |\n|    Inception-ResNet-v2     |      82.4         |       96.3        |      AAAI2016      |\n|Inceptionv4 + Inception-ResNet-v2|      83.5         |       96.92       |      AAAI2016      |\n|           RiR              |       -           |         -         |  ICLR Workshop2016 |\n|  Stochastic Depth ResNet   |      78.02        |         -         |      ECCV2016      |\n|           WRN              |      78.1         |       94.21       |      BMVC2016      |\n|       SqueezeNet           |      60.4         |       82.5        |      arXiv2017([rejected by ICLR2017](https://openreview.net/forum?id=S1xh5sYgx))     |\n|          GeNet             |      72.13        |       90.26       |      ICCV2017      |\n|         MetaQNN            |       -           |         -         |      ICLR2017      |\n|        PyramidNet          |      80.8         |       95.3        |      CVPR2017      |\n|         DenseNet           |      79.2         |       94.71       |      ECCV2017      |\n|        FractalNet          |      75.8         |       92.61       |      ICLR2017      |\n|         ResNext            |       -           |       96.97       |      CVPR2017      |\n|         IGCV1              |      73.05        |       91.08       |      ICCV2017      |\n| Residual Attention Network |      80.5         |       95.2        |      CVPR2017      |\n|        Xception            |       79          |       94.5        |      CVPR2017      |\n|        MobileNet           |      70.6         |         -         |      arXiv2017     |\n|         PolyNet            |      82.64        |       96.55       |      CVPR2017      |\n|           DPN              |       79          |       94.5        |      NIPS2017      |\n|        Block-QNN           |      77.4         |       93.54       |      CVPR2018      |\n|         CRU-Net            |      79.7         |       94.7        |      IJCAI2018     |\n|       DLA                  |      75.3         |         -         |      CVPR2018      |\n|       ShuffleNet           |      75.3         |         -         |      CVPR2018      |\n|       CondenseNet          |      73.8         |       91.7        |      CVPR2018      |\n|          NasNet            |      82.7         |       96.2        |      CVPR2018      |\n|       MobileNetV2          |      74.7         |         -         |      CVPR2018      |\n|         IGCV2              |      70.07        |         -         |      CVPR2018      |\n|          hier              |      79.7         |       94.8        |      ICLR2018      |\n|         PNasNet            |      82.9         |       96.2        |      ECCV2018      |\n|        AmoebaNet           |      83.9         |       96.6        |      AAAI2018      |\n|          SENet             |       -           |       97.749      |      CVPR2018      |\n|       ShuffleNetV2         |      81.44        |         -         |      ECCV2018      |\n|       CBAM                 |      79.93        |         94.41     |      ECCV2018      |\n|          IGCV3             |      72.2         |         -         |      BMVC2018      |\n|          BAM               |      77.56        |       93.71       |      BMVC2018      |\n|         MnasNet            |      76.13        |       92.85       |      CVPR2018      |\n|          SKNet             |      80.60        |         -         |      CVPR2019      |\n|          DARTS             |      73.3         |       91.3        |      ICLR2019      |\n|       ProxylessNAS         |      75.1         |       92.5        |      ICLR2019      |\n|       MobileNetV3          |      75.2         |         -         |      CVPR2019      |\n|          Res2Net           |      79.2         |       94.37       |      PAMI2019      |\n|       LIP-ResNet           |      79.33        |       94.6        |      ICCV2019      |\n|       EfficientNet         |      84.3         |       97.0        |      ICML2019      |\n|       FixResNeXt           |      86.4         |       98.0        |      NIPS2019      |\n|       BiT                  |      87.5         |         -         |      ECCV2020      |\n|       PSConv + ResNext101  |      80.502       |       95.276      |      ECCV2020      |\n|       NoisyStudent         |      88.4         |       98.7        |      CVPR2020      |\n|       RegNet               |      79.9         |       -           |      CVPR2020      |\n|       GhostNet             |      75.7         |       -           |      CVPR2020      |\n|       ViT                  |      88.55        |       -           |      ICLR2021      |\n|       DeiT                 |      85.2         |       -           |      ICML2021      |\n|       PVT                  |      81.7         |       -           |      ICCV2021      |\n|       T2T-Vit              |      83.3         |       -           |      ICCV2021      |\n|       DeepVit              |      80.9         |       -           |      Arvix2021     |\n|       ViL                  |      83.7         |       -           |      ICCV2021      |\n|       TNT                  |      83.9         |       -           |      Arvix2021     |\n|       CvT                  |      87.7         |       -           |      ICCV2021      |\n|       CViT                 |      84.1         |       -           |      ICCV2021      |\n|       Focal-T              |      84.0         |       -           |      NIPS2021      |\n|       Twins                |      83.7         |       -           |      NIPS2021      |\n|       PVTv2                |      81.7         |       -           |      CVM2022       |\n\n\n## Papers&Codes\n\n### VGG\n**Very Deep Convolutional Networks for Large-Scale Image Recognition.**\nKaren Simonyan, Andrew Zisserman\n- pdf: [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)\n- code: [torchvision : https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py](https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py)\n- code: [keras-applications : https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py](https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py)\n- code: [keras-applications : https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg19.py](https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg19.py)\n\n### GoogleNet\n**Going Deeper with Convolutions**\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich\n- pdf: [https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)\n- code: [unofficial-tensorflow : https://github.com/conan7882/GoogLeNet-Inception](https://github.com/conan7882/GoogLeNet-Inception)\n- code: [unofficial-caffe : https://github.com/lim0606/caffe-googlenet-bn](https://github.com/lim0606/caffe-googlenet-bn)\n\n### PReLU-nets\n**Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification**\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n- pdf: [https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852)\n- code: [unofficial-chainer : https://github.com/nutszebra/prelu_net](https://github.com/nutszebra/prelu_net)\n\n### ResNet\n**Deep Residual Learning for Image Recognition**\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n- pdf: [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)\n- code: [facebook-torch : https://github.com/facebook/fb.resnet.torch](https://github.com/facebook/fb.resnet.torch)\n- code: [torchvision : https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py)\n- code: [keras-applications : https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet.py](https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet.py)\n- code: [unofficial-keras : https://github.com/raghakot/keras-resnet](https://github.com/raghakot/keras-resnet)\n- code: [unofficial-tensorflow : https://github.com/ry/tensorflow-resnet](https://github.com/ry/tensorflow-resnet)\n\n### PreActResNet\n**Identity Mappings in Deep Residual Networks**\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n- pdf: [https://arxiv.org/abs/1603.05027](https://arxiv.org/abs/1603.05027)\n- code: [facebook-torch : https://github.com/facebook/fb.resnet.torch/blob/master/models/preresnet.lua](https://github.com/facebook/fb.resnet.torch/blob/master/models/preresnet.lua)\n- code: [official : https://github.com/KaimingHe/resnet-1k-layers](https://github.com/KaimingHe/resnet-1k-layers)\n- code: [unoffical-pytorch : https://github.com/kuangliu/pytorch-cifar/blob/master/models/preact_resnet.py](https://github.com/kuangliu/pytorch-cifar/blob/master/models/preact_resnet.py)\n- code: [unoffical-mxnet : https://github.com/tornadomeet/ResNet](https://github.com/tornadomeet/ResNet)\n\n### Inceptionv3\n**Rethinking the Inception Architecture for Computer Vision**\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna\n- pdf: [https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)\n- code: [torchvision : https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py](https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py)\n- code: [keras-applications : https://github.com/keras-team/keras-applications/blob/master/keras_applications/inception_v3.py](https://github.com/keras-team/keras-applications/blob/master/keras_applications/inception_v3.py)\n\n### Inceptionv4 && Inception-ResNetv2\n**Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning**\nChristian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n- pdf: [https://arxiv.org/abs/1602.07261](https://arxiv.org/abs/1602.07261)\n- code: [unofficial-keras : https://github.com/kentsommer/keras-inceptionV4](https://github.com/kentsommer/keras-inceptionV4)\n- code: [unofficial-keras : https://github.com/titu1994/Inception-v4](https://github.com/titu1994/Inception-v4)\n- code: [unofficial-keras : https://github.com/yuyang-huang/keras-inception-resnet-v2](https://github.com/yuyang-huang/keras-inception-resnet-v2)\n\n### RiR\n**Resnet in Resnet: Generalizing Residual Architectures**\nSasha Targ, Diogo Almeida, Kevin Lyman\n- pdf: [https://arxiv.org/abs/1603.08029](https://arxiv.org/abs/1603.08029)\n- code: [unofficial-tensorflow : https://github.com/SunnerLi/RiR-Tensorflow](https://github.com/SunnerLi/RiR-Tensorflow)\n- code: [unofficial-chainer : https://github.com/nutszebra/resnet_in_resnet](https://github.com/nutszebra/resnet_in_resnet)\n\n### Stochastic Depth ResNet\n**Deep Networks with Stochastic Depth**\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Weinberger\n- pdf: [https://arxiv.org/abs/1603.09382](https://arxiv.org/abs/1603.09382)\n- code: [unofficial-torch : https://github.com/yueatsprograms/Stochastic_Depth](https://github.com/yueatsprograms/Stochastic_Depth)\n- code: [unofficial-chainer : https://github.com/yasunorikudo/chainer-ResDrop](https://github.com/yasunorikudo/chainer-ResDrop)\n- code: [unofficial-keras : https://github.com/dblN/stochastic_depth_keras](https://github.com/dblN/stochastic_depth_keras)\n\n### WRN\n**Wide Residual Networks**\nSergey Zagoruyko, Nikos Komodakis\n- pdf: [https://arxiv.org/abs/1605.07146](https://arxiv.org/abs/1605.07146)\n- code: [official : https://github.com/szagoruyko/wide-residual-networks](https://github.com/szagoruyko/wide-residual-networks)\n- code: [unofficial-pytorch : https://github.com/xternalz/WideResNet-pytorch](https://github.com/xternalz/WideResNet-pytorch)\n- code: [unofficial-keras : https://github.com/asmith26/wide_resnets_keras](https://github.com/asmith26/wide_resnets_keras)\n- code: [unofficial-pytorch : https://github.com/meliketoy/wide-resnet.pytorch](https://github.com/meliketoy/wide-resnet.pytorch)\n\n### SqueezeNet\n**SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size**\nForrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer\n- pdf: [https://arxiv.org/abs/1602.07360](https://arxiv.org/abs/1602.07360)\n- code: [torchvision : https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py](https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py)\n- code: [unofficial-caffe : https://github.com/DeepScale/SqueezeNet](https://github.com/DeepScale/SqueezeNet)\n- code: [unofficial-keras : https://github.com/rcmalli/keras-squeezenet](https://github.com/rcmalli/keras-squeezenet)\n- code: [unofficial-caffe : https://github.com/songhan/SqueezeNet-Residual](https://github.com/songhan/SqueezeNet-Residual)\n\n### GeNet\n**Genetic CNN**\nLingxi Xie, Alan Yuille\n- pdf: [https://arxiv.org/abs/1703.01513](https://arxiv.org/abs/1703.01513)\n- code: [unofficial-tensorflow : https://github.com/aqibsaeed/Genetic-CNN](https://github.com/aqibsaeed/Genetic-CNN)\n\n### MetaQNN\n**Designing Neural Network Architectures using Reinforcement Learning**\nBowen Baker, Otkrist Gupta, Nikhil Naik, Ramesh Raskar\n- pdf: [https://arxiv.org/abs/1611.02167](https://arxiv.org/abs/1611.02167)\n- code: [official : https://github.com/bowenbaker/metaqnn](https://github.com/bowenbaker/metaqnn)\n\n### PyramidNet\n**Deep Pyramidal Residual Networks**\nDongyoon Han, Jiwhan Kim, Junmo Kim\n- pdf: [https://arxiv.org/abs/1610.02915](https://arxiv.org/abs/1610.02915)\n- code: [official : https://github.com/jhkim89/PyramidNet](https://github.com/jhkim89/PyramidNet)\n- code: [unofficial-pytorch : https://github.com/dyhan0920/PyramidNet-PyTorch](https://github.com/dyhan0920/PyramidNet-PyTorch)\n\n### DenseNet\n**Densely Connected Convolutional Networks**\nGao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger\n- pdf: [https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)\n- code: [official : https://github.com/liuzhuang13/DenseNet](https://github.com/liuzhuang13/DenseNet)\n- code: [unofficial-keras : https://github.com/titu1994/DenseNet](https://github.com/titu1994/DenseNet)\n- code: [unofficial-caffe : https://github.com/shicai/DenseNet-Caffe](https://github.com/shicai/DenseNet-Caffe)\n- code: [unofficial-tensorflow : https://github.com/YixuanLi/densenet-tensorflow](https://github.com/YixuanLi/densenet-tensorflow)\n- code: [unofficial-pytorch : https://github.com/YixuanLi/densenet-tensorflow](https://github.com/YixuanLi/densenet-tensorflow)\n- code: [unofficial-pytorch : https://github.com/bamos/densenet.pytorch](https://github.com/bamos/densenet.pytorch)\n- code: [unofficial-keras : https://github.com/flyyufelix/DenseNet-Keras](https://github.com/flyyufelix/DenseNet-Keras)\n\n### FractalNet\n**FractalNet: Ultra-Deep Neural Networks without Residuals**\nGustav Larsson, Michael Maire, Gregory Shakhnarovich\n- pdf: [https://arxiv.org/abs/1605.07648](https://arxiv.org/abs/1605.07648)\n- code: [unofficial-caffe : https://github.com/gustavla/fractalnet](https://github.com/gustavla/fractalnet)\n- code: [unofficial-keras : https://github.com/snf/keras-fractalnet](https://github.com/snf/keras-fractalnet)\n- code: [unofficial-tensorflow : https://github.com/tensorpro/FractalNet](https://github.com/tensorpro/FractalNet)\n\n### ResNext\n**Aggregated Residual Transformations for Deep Neural Networks**\nSaining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He\n- pdf: [https://arxiv.org/abs/1611.05431](https://arxiv.org/abs/1611.05431)\n- code: [official : https://github.com/facebookresearch/ResNeXt](https://github.com/facebookresearch/ResNeXt)\n- code: [keras-applications : https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnext.py](https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnext.py)\n- code: [unofficial-pytorch : https://github.com/prlz77/ResNeXt.pytorch](https://github.com/prlz77/ResNeXt.pytorch)\n- code: [unofficial-keras : https://github.com/titu1994/Keras-ResNeXt](https://github.com/titu1994/Keras-ResNeXt)\n- code: [unofficial-tensorflow : https://github.com/taki0112/ResNeXt-Tensorflow](https://github.com/taki0112/ResNeXt-Tensorflow)\n- code: [unofficial-tensorflow : https://github.com/wenxinxu/ResNeXt-in-tensorflow](https://github.com/wenxinxu/ResNeXt-in-tensorflow)\n\n### IGCV1\n**Interleaved Group Convolutions for Deep Neural Networks**\nTing Zhang, Guo-Jun Qi, Bin Xiao, Jingdong Wang\n- pdf: [https://arxiv.org/abs/1707.02725](https://arxiv.org/abs/1707.02725)\n- code [official : https://github.com/hellozting/InterleavedGroupConvolutions](https://github.com/hellozting/InterleavedGroupConvolutions)\n\n### Residual Attention Network\n**Residual Attention Network for Image Classification**\nFei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang\n- pdf: [https://arxiv.org/abs/1704.06904](https://arxiv.org/abs/1704.06904)\n- code: [official : https://github.com/fwang91/residual-attention-network](https://github.com/fwang91/residual-attention-network)\n- code: [unofficial-pytorch : https://github.com/tengshaofeng/ResidualAttentionNetwork-pytorch](https://github.com/tengshaofeng/ResidualAttentionNetwork-pytorch)\n- code: [unofficial-gluon : https://github.com/PistonY/ResidualAttentionNetwork](https://github.com/PistonY/ResidualAttentionNetwork)\n- code: [unofficial-keras : https://github.com/koichiro11/residual-attention-network](https://github.com/koichiro11/residual-attention-network)\n\n### Xception\n**Xception: Deep Learning with Depthwise Separable Convolutions**\nFran\u00e7ois Chollet\n- pdf: [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)\n- code: [unofficial-pytorch : https://github.com/jfzhang95/pytorch-deeplab-xception/blob/master/modeling/backbone/xception.py](https://github.com/jfzhang95/pytorch-deeplab-xception/blob/master/modeling/backbone/xception.py)\n- code: [unofficial-tensorflow : https://github.com/kwotsin/TensorFlow-Xception](https://github.com/kwotsin/TensorFlow-Xception)\n- code: [unofficial-caffe : https://github.com/yihui-he/Xception-caffe](https://github.com/yihui-he/Xception-caffe)\n- code: [unofficial-pytorch : https://github.com/tstandley/Xception-PyTorch](https://github.com/tstandley/Xception-PyTorch)\n- code: [keras-applications : https://github.com/keras-team/keras-applications/blob/master/keras_applications/xception.py](https://github.com/keras-team/keras-applications/blob/master/keras_applications/xception.py)\n\n### MobileNet\n**MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications**\nAndrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam\n- pdf: [https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)\n- code: [unofficial-tensorflow : https://github.com/Zehaos/MobileNet](https://github.com/Zehaos/MobileNet)\n- code: [unofficial-caffe : https://github.com/shicai/MobileNet-Caffe](https://github.com/shicai/MobileNet-Caffe)\n- code: [unofficial-pytorch : https://github.com/marvis/pytorch-mobilenet](https://github.com/marvis/pytorch-mobilenet)\n- code: [keras-applications : https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py](https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py)\n\n### PolyNet\n**PolyNet: A Pursuit of Structural Diversity in Very Deep Networks**\nXingcheng Zhang, Zhizhong Li, Chen Change Loy, Dahua Lin\n- pdf: [https://arxiv.org/abs/1611.05725](https://arxiv.org/abs/1611.05725)\n- code: [official : https://github.com/open-mmlab/polynet](https://github.com/open-mmlab/polynet)\n\n### DPN\n**Dual Path Networks**\nYunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, Jiashi Feng\n- pdf: [https://arxiv.org/abs/1707.01629](https://arxiv.org/abs/1707.01629)\n- code: [official : https://github.com/cypw/DPNs](https://github.com/cypw/DPNs)\n- code: [unoffical-keras : https://github.com/titu1994/Keras-DualPathNetworks](https://github.com/titu1994/Keras-DualPathNetworks)\n- code: [unofficial-pytorch : https://github.com/oyam/pytorch-DPNs](https://github.com/oyam/pytorch-DPNs)\n- code: [unofficial-pytorch : https://github.com/rwightman/pytorch-dpn-pretrained](https://github.com/rwightman/pytorch-dpn-pretrained)\n\n### Block-QNN\n**Practical Block-wise Neural Network Architecture Generation**\nZhao Zhong, Junjie Yan, Wei Wu, Jing Shao, Cheng-Lin Liu\n- pdf: [https://arxiv.org/abs/1708.05552](https://arxiv.org/abs/1708.05552)\n\n### CRU-Net\n**Sharing Residual Units Through Collective Tensor Factorization in Deep Neural Networks**\nChen Yunpeng, Jin Xiaojie, Kang Bingyi, Feng Jiashi, Yan Shuicheng\n- pdf: [https://arxiv.org/abs/1703.02180](https://arxiv.org/abs/1703.02180)\n- code [official : https://github.com/cypw/CRU-Net](https://github.com/cypw/CRU-Net)\n- code [unofficial-mxnet : https://github.com/bruinxiong/Modified-CRUNet-and-Residual-Attention-Network.mxnet](https://github.com/bruinxiong/Modified-CRUNet-and-Residual-Attention-Network.mxnet)\n\n## DLA\n**Deep Layer Aggregation**\nFisher Yu, Dequan Wang, Evan Shelhamer, Trevor Darrell\n- pdf: [https://arxiv.org/abs/1707.06484](https://arxiv.org/abs/1707.06484)\n- code: [official-pytorch: https://github.com/ucbdrive/dla](https://github.com/ucbdrive/dla)\n\n### ShuffleNet\n**ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices**\nXiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun\n- pdf: [https://arxiv.org/abs/1707.01083](https://arxiv.org/abs/1707.01083)\n- code: [unofficial-tensorflow : https://github.com/MG2033/ShuffleNet](https://github.com/MG2033/ShuffleNet)\n- code: [unofficial-pytorch : https://github.com/jaxony/ShuffleNet](https://github.com/jaxony/ShuffleNet)\n- code: [unofficial-caffe : https://github.com/farmingyard/ShuffleNet](https://github.com/farmingyard/ShuffleNet)\n- code: [unofficial-keras : https://github.com/scheckmedia/keras-shufflenet](https://github.com/scheckmedia/keras-shufflenet)\n\n### CondenseNet\n**CondenseNet: An Efficient DenseNet using Learned Group Convolutions**\nGao Huang, Shichen Liu, Laurens van der Maaten, Kilian Q. Weinberger\n- pdf: [https://arxiv.org/abs/1711.09224](https://arxiv.org/abs/1711.09224)\n- code: [official : https://github.com/ShichenLiu/CondenseNet](https://github.com/ShichenLiu/CondenseNet)\n- code: [unofficial-tensorflow : https://github.com/markdtw/condensenet-tensorflow](https://github.com/markdtw/condensenet-tensorflow)\n\n### NasNet\n**Learning Transferable Architectures for Scalable Image Recognition**\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le\n- pdf: [https://arxiv.org/abs/1707.07012](https://arxiv.org/abs/1707.07012)\n- code: [unofficial-keras : https://github.com/titu1994/Keras-NASNet](https://github.com/titu1994/Keras-NASNet)\n- code: [keras-applications : https://github.com/keras-team/keras-applications/blob/master/keras_applications/nasnet.py](https://github.com/keras-team/keras-applications/blob/master/keras_applications/nasnet.py)\n- code: [unofficial-pytorch : https://github.com/wandering007/nasnet-pytorch](https://github.com/wandering007/nasnet-pytorch)\n- code: [unofficial-tensorflow : https://github.com/yeephycho/nasnet-tensorflow](https://github.com/yeephycho/nasnet-tensorflow)\n\n### MobileNetV2\n**MobileNetV2: Inverted Residuals and Linear Bottlenecks**\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen\n- pdf: [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)\n- code: [unofficial-keras : https://github.com/xiaochus/MobileNetV2](https://github.com/xiaochus/MobileNetV2)\n- code: [unofficial-pytorch : https://github.com/Randl/MobileNetV2-pytorch](https://github.com/Randl/MobileNetV2-pytorch)\n- code: [unofficial-tensorflow : https://github.com/neuleaf/MobileNetV2](https://github.com/neuleaf/MobileNetV2)\n\n### IGCV2\n**IGCV2: Interleaved Structured Sparse Convolutional Neural Networks**\nGuotian Xie, Jingdong Wang, Ting Zhang, Jianhuang Lai, Richang Hong, Guo-Jun Qi\n- pdf: [https://arxiv.org/abs/1804.06202](https://arxiv.org/abs/1804.06202)\n\n### hier\n**Hierarchical Representations for Efficient Architecture Search**\nHanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray Kavukcuoglu\n- pdf: [https://arxiv.org/abs/1711.00436](https://arxiv.org/abs/1711.00436)\n\n### PNasNet\n**Progressive Neural Architecture Search**\nChenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, Kevin Murphy\n- pdf: [https://arxiv.org/abs/1712.00559](https://arxiv.org/abs/1712.00559)\n- code: [tensorflow-slim : https://github.com/tensorflow/models/blob/master/research/slim/nets/nasnet/pnasnet.py](https://github.com/tensorflow/models/blob/master/research/slim/nets/nasnet/pnasnet.py)\n- code: [unofficial-pytorch : https://github.com/chenxi116/PNASNet.pytorch](https://github.com/chenxi116/PNASNet.pytorch)\n- code: [unofficial-tensorflow : https://github.com/chenxi116/PNASNet.TF](https://github.com/chenxi116/PNASNet.TF)\n\n### AmoebaNet\n**Regularized Evolution for Image Classifier Architecture Search**\nEsteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le\n- pdf: [https://arxiv.org/abs/1802.01548](https://arxiv.org/abs/1802.01548)\n- code: [tensorflow-tpu : https://github.com/tensorflow/tpu/tree/master/models/official/amoeba_net](https://github.com/tensorflow/tpu/tree/master/models/official/amoeba_net)\n\n### SENet\n**Squeeze-and-Excitation Networks**\nJie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu\n- pdf: [https://arxiv.org/abs/1709.01507](https://arxiv.org/abs/1709.01507)\n- code: [official : https://github.com/hujie-frank/SENet](https://github.com/hujie-frank/SENet)\n- code: [unofficial-pytorch : https://github.com/moskomule/senet.pytorch](https://github.com/moskomule/senet.pytorch)\n- code: [unofficial-tensorflow : https://github.com/taki0112/SENet-Tensorflow](https://github.com/taki0112/SENet-Tensorflow)\n- code: [unofficial-caffe : https://github.com/shicai/SENet-Caffe](https://github.com/shicai/SENet-Caffe)\n- code: [unofficial-mxnet : https://github.com/bruinxiong/SENet.mxnet](https://github.com/bruinxiong/SENet.mxnet)\n\n### ShuffleNetV2\n**ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design**\nNingning Ma, Xiangyu Zhang, Hai-Tao Zheng, Jian Sun\n- pdf: [https://arxiv.org/abs/1807.11164](https://arxiv.org/abs/1807.11164)\n- code: [unofficial-pytorch : https://github.com/Randl/ShuffleNetV2-pytorch](https://github.com/Randl/ShuffleNetV2-pytorch)\n- code: [unofficial-keras : https://github.com/opconty/keras-shufflenetV2](https://github.com/opconty/keras-shufflenetV2)\n- code: [unofficial-pytorch : https://github.com/Bugdragon/ShuffleNet_v2_PyTorch](https://github.com/Bugdragon/ShuffleNet_v2_PyTorch)\n- code: [unofficial-caff2: https://github.com/wolegechu/ShuffleNetV2.Caffe2](https://github.com/wolegechu/ShuffleNetV2.Caffe2)\n\n### CBAM\nCBAM: Convolutional Block Attention Module\nSanghyun Woo, Jongchan Park, Joon-Young Lee, In So Kweon\n- pdf: [https://arxiv.org/abs/1807.06521](https://arxiv.org/abs/1807.06521)\n- code: [official-pytorch : https://github.com/Jongchan/attention-module](https://github.com/Jongchan/attention-module)\n- code: [unofficial-pytorch : https://github.com/luuuyi/CBAM.PyTorch](https://github.com/luuuyi/CBAM.PyTorch)\n- code: [unofficial-pytorch : https://github.com/elbuco1/CBAM](https://github.com/elbuco1/CBAM)\n- code: [unofficial-keras : https://github.com/kobiso/CBAM-keras](https://github.com/kobiso/CBAM-keras)\n\n\n### IGCV3\n**IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks**\nKe Sun, Mingjie Li, Dong Liu, Jingdong Wang\n- pdf: [https://arxiv.org/abs/1806.00178](https://arxiv.org/abs/1806.00178)\n- code: [official : https://github.com/homles11/IGCV3](https://github.com/homles11/IGCV3)\n- code: [unofficial-pytorch : https://github.com/xxradon/IGCV3-pytorch](https://github.com/xxradon/IGCV3-pytorch)\n- code: [unofficial-tensorflow : https://github.com/ZHANG-SHI-CHANG/IGCV3](https://github.com/ZHANG-SHI-CHANG/IGCV3)\n\n### BAM\n**BAM: Bottleneck Attention Module**\nJongchan Park, Sanghyun Woo, Joon-Young Lee, In So Kweon\n- pdf: [https://arxiv.org/abs/1807.06514](https://arxiv.org/abs/1807.06514)\n- code: [official-pytorch : https://github.com/Jongchan/attention-module](https://github.com/Jongchan/attention-module)\n- code: [unofficial-tensorflow : https://github.com/huyz1117/BAM](https://github.com/huyz1117/BAM)\n\n### MNasNet\n**MnasNet: Platform-Aware Neural Architecture Search for Mobile**\nMingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Quoc V. Le\n- pdf: [https://arxiv.org/abs/1807.11626](https://arxiv.org/abs/1807.11626)\n- code: [unofficial-pytorch : https://github.com/AnjieZheng/MnasNet-PyTorch](https://github.com/AnjieZheng/MnasNet-PyTorch)\n- code: [unofficial-caffe : https://github.com/LiJianfei06/MnasNet-caffe](https://github.com/LiJianfei06/MnasNet-caffe)\n- code: [unofficial-MxNet : https://github.com/chinakook/Mnasnet.MXNet](https://github.com/chinakook/Mnasnet.MXNet)\n- code: [unofficial-keras : https://github.com/Shathe/MNasNet-Keras-Tensorflow](https://github.com/Shathe/MNasNet-Keras-Tensorflow)\n\n### SKNet\n**Selective Kernel Networks**\nXiang Li, Wenhai Wang, Xiaolin Hu, Jian Yang\n- pdf: [https://arxiv.org/abs/1903.06586](https://arxiv.org/abs/1903.06586)\n- code: [official : https://github.com/implus/SKNet](https://github.com/implus/SKNet)\n\n### DARTS\n**DARTS: Differentiable Architecture Search**\nHanxiao Liu, Karen Simonyan, Yiming Yang\n- pdf: [https://arxiv.org/abs/1806.09055](https://arxiv.org/abs/1806.09055)\n- code: [official : https://github.com/quark0/darts](https://github.com/quark0/darts)\n- code: [unofficial-pytorch : https://github.com/khanrc/pt.darts](https://github.com/khanrc/pt.darts)\n- code: [unofficial-tensorflow : https://github.com/NeroLoh/darts-tensorflow](https://github.com/NeroLoh/darts-tensorflow)\n\n### ProxylessNAS\n**ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware**\nHan Cai, Ligeng Zhu, Song Han\n- pdf: [https://arxiv.org/abs/1812.00332](https://arxiv.org/abs/1812.00332)\n- code: [official : https://github.com/mit-han-lab/ProxylessNAS](https://github.com/mit-han-lab/ProxylessNAS)\n\n### MobileNetV3\n**Searching for MobileNetV3**\nAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam\n- pdf: [https://arxiv.org/abs/1905.02244](https://arxiv.org/abs/1905.02244)\n- code: [unofficial-pytorch : https://github.com/xiaolai-sqlai/mobilenetv3](https://github.com/xiaolai-sqlai/mobilenetv3)\n- code: [unofficial-pytorch : https://github.com/kuan-wang/pytorch-mobilenet-v3](https://github.com/kuan-wang/pytorch-mobilenet-v3)\n- code: [unofficial-pytorch : https://github.com/leaderj1001/MobileNetV3-Pytorch](https://github.com/leaderj1001/MobileNetV3-Pytorch)\n- code: [unofficial-pytorch : https://github.com/d-li14/mobilenetv3.pytorch](https://github.com/d-li14/mobilenetv3.pytorch)\n- code: [unofficial-caffe : https://github.com/jixing0415/caffe-mobilenet-v3](https://github.com/jixing0415/caffe-mobilenet-v3)\n- code: [unofficial-keras : https://github.com/xiaochus/MobileNetV3](https://github.com/xiaochus/MobileNetV3)\n\n### Res2Net\n**Res2Net: A New Multi-scale Backbone Architecture**\nShang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, Philip Torr\n- pdf: [https://arxiv.org/abs/1904.01169](https://arxiv.org/abs/1904.01169)\n- code: [unofficial-pytorch : https://github.com/4uiiurz1/pytorch-res2net](https://github.com/4uiiurz1/pytorch-res2net)\n- code: [unofficial-keras : https://github.com/fupiao1998/res2net-keras](https://github.com/fupiao1998/res2net-keras)\n- code: [official-pytorch : https://github.com/Res2Net](https://github.com/Res2Net)\n\n### LIP-ResNet\n**LIP: Local Importance-based Pooling**\nZiteng Gao, Limin Wang, Gangshan Wu\n- pdf: [https://arxiv.org/abs/1908.04156](https://arxiv.org/abs/1908.04156)\n- code: [official-pytorch : https://github.com/sebgao/LIP](https://github.com/sebgao/LIP)\n\n### EfficientNet\n\n**EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks**\nMingxing Tan, Quoc V. Le\n- pdf: [https://arxiv.org/abs/1905.11946](https://arxiv.org/abs/1905.11946)\n- code: [unofficial-pytorch : https://github.com/lukemelas/EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch)\n- code: [official-tensorflow : https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet)\n\n\n### FixResNeXt \n**Fixing the train-test resolution discrepancy**\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, Herv\u00e9 J\u00e9gou\n- pdf: [https://arxiv.org/abs/1906.06423](https://arxiv.org/abs/1906.06423)\n- code: [official-pytorch : https://github.com/facebookresearch/FixRes](https://github.com/facebookresearch/FixRes)\n\n\n### BiT\n**Big Transfer (BiT): General Visual Representation Learning**\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby\n- pdf: [https://arxiv.org/abs/1912.11370](https://arxiv.org/abs/1912.11370)\n- code: [official-tensorflow: https://github.com/google-research/big_transfer](https://github.com/google-research/big_transfer)\n\n### PSConv + ResNext101\n**PSConv: Squeezing Feature Pyramid into One Compact Poly-Scale Convolutional Layer**\nDuo Li1, Anbang Yao2B, and Qifeng Chen1B\n- pdf: [https://arxiv.org/abs/2007.06191](https://arxiv.org/abs/2007.06191)\n- code: [https://github.com/d-li14/PSConv](https://github.com/d-li14/PSConv)\n\n\n### NoisyStudent\n**Self-training with Noisy Student improves ImageNet classification**\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V. Le\n- pdf: [https://arxiv.org/abs/1911.04252](https://arxiv.org/abs/1911.04252)\n- code: [official-tensorflow: https://github.com/google-research/noisystudent](https://github.com/google-research/noisystudent)\n- code: [unofficial-pytorch: https://github.com/sally20921/NoisyStudent](https://github.com/sally20921/NoisyStudent)\n\n### RegNet\n**Designing Network Design Spaces**\nIlija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Doll\u00e1r\n- pdf: [https://arxiv.org/abs/2003.13678](https://arxiv.org/abs/2003.13678)\n- code: [official-pytorch: https://github.com/facebookresearch/pycls](https://github.com/facebookresearch/pycls)\n- code: [unofficial-pytorch: https://github.com/d-li14/regnet.pytorch](https://github.com/d-li14/regnet.pytorch)\n\n### GhostNet\n**GhostNet: More Features from Cheap Operations**\nKai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, Chang Xu\n- pdf: [https://arxiv.org/abs/1911.11907](https://arxiv.org/abs/1911.11907)\n- code: [official-pytorch: https://github.com/huawei-noah/ghostnet](https://github.com/huawei-noah/ghostnet)\n\n### ViT\n**An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale**\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby\n- pdf: [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)\n- code: [official-tensorflow: https://github.com/google-research/vision_transformer](https://github.com/google-research/vision_transformer)\n- code: [unofficial-pytorch: https://github.com/jeonsworld/ViT-pytorch](https://github.com/jeonsworld/ViT-pytorch)\n\n### DeiT\n**Training data-efficient image transformers & distillation through attention**\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou\n- pdf: [https://arxiv.org/abs/2012.12877](https://arxiv.org/abs/2012.12877)\n- code: [official-pytorch: https://github.com/facebookresearch/deit](https://github.com/facebookresearch/deit)\n\n### PVT\n**Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions**\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao\n- pdf: [https://arxiv.org/abs/2102.12122](https://arxiv.org/abs/2102.12122)\n- code: [official-pytorch: https://github.com/whai362/PVT](https://github.com/whai362/PVT)\n\n### T2T\n**Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet**\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, Shuicheng Yan\n- pdf: [https://arxiv.org/abs/2101.11986](https://arxiv.org/abs/2101.11986)\n- code: [official-pytorch: https://github.com/yitu-opensource/T2T-ViT](https://github.com/yitu-opensource/T2T-ViT)\n\n### DeepVit\n**DeepViT: Towards Deeper Vision Transformer**\nDaquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng.\n- pdf: [https://arxiv.org/abs/2103.11886](https://arxiv.org/abs/2103.11886)\n- code: [official-pytorch: https://github.com/zhoudaquan/dvit_repo](https://github.com/zhoudaquan/dvit_repo)\n\n### ViL\n**Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding**\nPengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, Jianfeng Gao\n- pdf: [https://arxiv.org/abs/2103.15358](https://arxiv.org/abs/2103.15358)\n- code: [official-pytorch: https://github.com/microsoft/vision-longformer](https://github.com/microsoft/vision-longformer)\n\n### TNT\n**Transformer in Transformer**\nKai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang\n- pdf: [https://arxiv.org/abs/2103.00112](https://arxiv.org/abs/2103.00112)\n- code: [https://github.com/huawei-noah/CV-Backbones](https://github.com/huawei-noah/CV-Backbones)\n\n### CvT\n**CvT: Introducing Convolutions to Vision Transformers**\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang\n- pdf: [https://arxiv.org/abs/2103.15808](https://arxiv.org/abs/2103.15808)\n- code: [https://github.com/microsoft/CvT](https://github.com/microsoft/CvT)\n\n### CViT\n**CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification**\nChun-Fu (Richard) Chen, Quanfu Fan, Rameswar Panda\n- pdf: [https://arxiv.org/abs/2103.14899](https://arxiv.org/abs/2103.14899)\n- code: [https://github.com/IBM/CrossViT](https://github.com/IBM/CrossViT)\n\n### Focal-T\n**Focal Attention for Long-Range Interactions in Vision Transformers**\nJianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, Jianfeng Gao\n- pdf: [https://arxiv.org/abs/2107.00641](https://arxiv.org/abs/2107.00641)\n- code: [ https://github.com/microsoft/Focal-Transformer](https://github.com/microsoft/Focal-Transformer)\n\n### Twins\n**Twins: Revisiting the Design of Spatial Attention in Vision Transformers**\n- pdf: [https://arxiv.org/abs/2104.13840](https://arxiv.org/abs/2104.13840)\n- code: [https://git.io/Twins]( https://git.io/Twins)\n\n### PVTv2\n**Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao**\n- pdf: [https://arxiv.org/abs/2106.13797](https://arxiv.org/abs/2106.13797)\n- code: [official-pytorch: https://github.com/whai362/PVT](https://github.com/whai362/PVT)"
}