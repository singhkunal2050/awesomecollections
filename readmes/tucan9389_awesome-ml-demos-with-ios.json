{
  "repo_name": "tucan9389_awesome-ml-demos-with-ios",
  "readme_content": "<p align=\"center\">\n<img src=\"Resource/awesome-ml-demos-with-ios-logo.png\" width=\"187\" height=\"174\"/>\n</p>\n\n\n[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/motlabs/awesome-ml-demos-with-ios) ![Hits](https://hitcounter.pythonanywhere.com/count/tag.svg?url=https%3A%2F%2Fgithub.com%2Fmotlabs%2Fawesome-ml-demos-with-ios) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) [![GIF PRs More Welcome](https://img.shields.io/badge/GIF--PRs-WELCOME!-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) \n\n> This repo was moved from [@motlabs](https://github.com/motlabs) group. Thanks for [@jwkanggist](https://github.com/jwkanggist) who is a leader of motlabs community.\n\n# Awesome Machine Learning DEMOs with iOS\n\nWe tackle the challenge of using machine learning models on iOS via Core ML and ML Kit (TensorFlow Lite).\n\n[\ud55c\uad6d\uc5b4 README](https://github.com/motlabs/iOS-Proejcts-with-ML-Models/blob/master/README_kr.md)\n\n## Contents\n- [Machine Learning Framework for iOS](#machine-learning-framework-for-ios)\n  - [Flow of Model When Using Core ML](#Flow-of-Model-When-Using-Core-ML)\n  - [Flow of Model When Using Create ML](#Flow-of-Model-When-Using-Create-ML)\n- [Baseline Projects](#Baseline-Projects)\n  - [Image Classification](#Image-Classification)\n  - [Object Detection & Recognition](#Object-Detection--Recognition)\n  - [Image Estimation](#Image-Estimation)\n  - [Semantic Segmentation](#Semantic-Segmentation)\n- [Application Projects](#Application-Projects)\n  - [Annotation Tool](#Annotation-Tool)\n- [Create ML Projects](#Create-ML-Projects)\n- [Performance](#Performance)\n  - [\ud83d\udccfMeasure module](#measure-module)\n  - [Implements](#Implements)\n- [See also](#See-also)\n\n## Machine Learning Framework for iOS\n\n- [Core ML](https://developer.apple.com/documentation/coreml)\n- [TensorFlow Lite](https://www.tensorflow.org/lite)\n- [Pytorch Mobile](https://pytorch.org/mobile/home/)\n- [fritz](https://www.fritz.ai/)\n- etc. ~~[Tensorflow Mobile](https://www.tensorflow.org/mobile/)~~`DEPRECATED`)\n\n\n### Flow of Model When Using Core ML\n\n[![Flow of Model When Using Core ML](Resource/flow_of_model_when_using_coreml.png?raw=true)](https://docs.google.com/presentation/d/1wA_PAjllpLLcFPuZcERYbQlPe1Ipb-bzIZinZg3zXkg/edit?usp=sharing)\n\nThe overall flow is very similar for most ML frameworks. Each framework has its own compatible model format. We need to take the model created in TensorFlow and **convert it into the appropriate format, for each mobile ML framework**.\n\nOnce the compatible model is prepared, you can run the inference using the ML framework. Note that you must perform **pre/postprocessing** manually.\n\n> If you want more explanation, check [this slide(Korean)](https://docs.google.com/presentation/d/1wA_PAjllpLLcFPuZcERYbQlPe1Ipb-bzIZinZg3zXkg/edit?usp=sharing).\n\n### Flow of Model When Using Create ML\n\n![playground-createml-validation-001](Resource/flow_of_model_when_using_createml.png)\n\n## Baseline Projects\n\n#### DONE\n\n- Using built-in model with Core ML\n\n- Using built-in on-device model with ML Kit\n- Using custom model for Vision with Core ML and ML Kit\n- Object Detection with Core ML\n\n#### TODO\n\n- Object Detection with ML Kit\n- Using built-in cloud model on ML Kit\n  - Landmark recognition\n- Using custom model for NLP with Core ML and ML Kit\n- Using custom model for Audio with Core ML and ML Kit\n  - Audio recognition\n  - Speech recognition\n  - TTS\n\n\n\n### Image Classification\n\n| Name | DEMO | Note |\n| ---- | ---- | ---- |\n| [ImageClassification-CoreML](https://github.com/tucan9389/ImageClassification-CoreML) | <p align=\"center\"><img src=\"Resource/MobileNet-CoreML-DEMO.gif\" width=\"200\"/></p> | - |\n| [MobileNet-MLKit](https://github.com/tucan9389/MobileNet-MLKit) | <p align=\"center\"><img src=\"Resource/MobileNet-MLKit-DEMO.gif\" width=\"200\"/></p> | - |\n\n### Object Detection & Recognition\n\n| Name | DEMO | Note |\n| ---- | ---- | ---- |\n| [ObjectDetection-CoreML](https://github.com/tucan9389/ObjectDetection-CoreML) | <p align=\"center\"><img src=\"Resource/SSDMobileNetV2-DEMO.gif\" width=\"200\"/></p> | - |\n| [TextDetection-CoreML](https://github.com/tucan9389/TextDetection-CoreML) | <p align=\"center\"><img src=\"Resource/TextDetection-CoreML_DEMO001.gif\" width=\"200\"/></p> | - |\n| [TextRecognition-MLKit](https://github.com/tucan9389/TextRecognition-MLKit) | <p align=\"center\"><img src=\"Resource/TextRecognition-MLKit_DEMO002.gif\" width=\"200\"/></p> | - |\n| [FaceDetection-MLKit](https://github.com/tucan9389/FaceDetection-MLKit) | <p align=\"center\"><img src=\"Resource/FaceDetection-MLKit-DEMO.gif\" width=\"200\"/></p> | - |\n\n### Pose Estimation\n\n| Name | DEMO | Note |\n| ---- | :--- | ---- |\n| [PoseEstimation-CoreML](https://github.com/tucan9389/PoseEstimation-CoreML) | <p align=\"center\"><img src=\"Resource/180801-poseestimation-demo.gif\" width=\"200\"/></p> | - |\n| [PoseEstimation-TFLiteSwift](https://github.com/tucan9389/PoseEstimation-TFLiteSwift) | <img src=\"https://user-images.githubusercontent.com/37643248/77227994-99ba2a80-6bc7-11ea-9b08-9bb57723bc42.gif\" width=200px><img src=\"https://user-images.githubusercontent.com/37643248/110994933-e68ca780-83bc-11eb-8331-d827e19d2d36.gif\" width=200px> | -    |\n| [PoseEstimation-MLKit](https://github.com/tucan9389/PoseEstimation-MLKit) | <p align=\"center\"><img src=\"Resource/PoseEstimation-MLKit-hourglass.gif\" width=\"200\"/></p> | -    |\n| [FingertipEstimation-CoreML](https://github.com/tucan9389/FingertipEstimation-CoreML) | <p align=\"center\"><img src=\"Resource/fingertip_estimation_demo003.gif\" width=\"200\"/></p> | - |\n\n### Depth Prediction\n\n|                                                              |                                                              |      |\n| ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |\n| [DepthPrediction-CoreML](https://github.com/tucan9389/DepthPrediction-CoreML) | <p align=\"center\"><img src=\"Resource/190727-depthprediction-demo001.gif\" width=\"200\"/></p> | -    |\n\n### Semantic Segmentation\n\n| Name | DEMO | Note |\n| ---- | ---- | ---- |\n| [SemanticSegmentation-CoreML](https://github.com/tucan9389/SemanticSegmentation-CoreML) | <p align=\"center\"><img src=\"https://user-images.githubusercontent.com/37643248/99242802-167ad280-2843-11eb-959a-5fe3b169d8f0.gif\" width=\"200\"/><img src=\"https://user-images.githubusercontent.com/37643248/110972921-e8943d80-839f-11eb-9559-2a32d3b56de0.gif\" width=200px></p> | - |\n\n## Application Projects\n\n| Name | DEMO | Note |\n| ---- | ---- | ---- |\n| [dont-be-turtle-ios](https://github.com/motlabs/dont-be-turtle-ios) | <p align=\"center\"><img src=\"Resource/dont-be-turtle_demo_004.gif\" width=\"200\"/></p> | - |\n| [WordRecognition-CoreML-MLKit](https://github.com/tucan9389/WordRecognition-CoreML-MLKit)(preparing...) | <p align=\"center\"><img src=\"Resource/recognition_a_word_demo002.gif\" width=\"200\"/></p> | Detect character, find a word what I point and then recognize the word using Core ML and ML Kit. |\n\n### Annotation Tool\n\n| Name | DEMO | Note |\n| ---- | ---- | ---- |\n| [KeypointAnnotation](https://github.com/tucan9389/KeypointAnnotation) | <p align=\"center\"><img src=\"Resource/annotation_ios_app_demo001.gif\" width=\"200\"/></p> | Annotation tool for own custom estimation dataset |\n\n## Create ML Projects\n\n| Name | Create ML DEMO | Core ML DEMO | Note |\n| ------ | ------------------------------------------------------------ | ---------------------------------- | ------ |\n| [SimpleClassification-CreateML-CoreML](https://github.com/tucan9389/SimpleClassification-CreateML-CoreML) | ![IMG_0436](Resource/playground-createml-validation-001.png) | ![IMG_0436](Resource/IMG_0436.PNG) | A Simple Classification Using Create ML and Core ML |\n\n## Performance\n\nExecution Time: Inference Time + Postprocessing Time\n\n|              (with iPhone X) | Inference Time(ms) | Execution Time(ms) |   FPS   |\n| ---------------------------: | :----------------: | :----------------: | :-----: |\n|   ImageClassification-CoreML |         40         |         40         |   23    |\n|              MobileNet-MLKit |        120         |        130         |    6    |\n|       ObjectDetection-CoreML |  100 ~ 120         |    110 ~ 130       |    5    |\n|         TextDetection-CoreML |         12         |         13         | 30(max) |\n|        TextRecognition-MLKit |       35~200       |       40~200       |  5~20   |\n|        PoseEstimation-CoreML |         51         |         65         |   14    |\n|         PoseEstimation-MLKit |        200         |        217         |    3    |\n|       DepthPrediction-CoreML |        624         |        640         |    1    |\n|    SemanticSegmentation-CoreML |        178         |        509         |    1    |\n| WordRecognition-CoreML-MLKit |         23         |         30         |   14    |\n| FaceDetection-MLKit          |         -          |          -         |   -     |\n\n### \ud83d\udccfMeasure module\n\nYou can see the measured latency time for inference or execution and FPS on the top of the screen.\n\n> If you have more elegant method for measuring the performance, suggest on issue!\n\n<img src=\"Resource/measure_ui.jpeg\" width=\"320\"/>\n\n### Implements\n\n|                            | Measure\ud83d\udccf | Unit Test | Bunch Test |\n| -------------------------: | :-------: | :-------: | :--------: |\n| ImageClassification-CoreML |    O      |     X     |     X      |\n|            MobileNet-MLKit |    O      |     X     |     X      |\n|     ObjectDetection-CoreML |    O      |     O     |     X      |\n|       TextDetection-CoreML |    O      |     X     |     X      |\n|      TextRecognition-MLKit |    O      |     X     |     X      |\n|      PoseEstimation-CoreML |    O      |     O     |     X      |\n|       PoseEstimation-MLKit |    O      |     X     |     X      |\n|     DepthPrediction-CoreML |    O      |     X     |     X      |\n|  SemanticSegmentation-CoreML |    O      |     X     |     X      |\n\n## See also\n\n- [Core ML | Apple Developer Documentation](https://developer.apple.com/documentation/coreml)\n- [Machine Learning - Apple Developer](https://developer.apple.com/machine-learning/)\n- [ML Kit - Firebase](https://developers.google.com/ml-kit/)\n- [Apple's Core ML 2 vs. Google's ML Kit: What's the difference?](https://venturebeat.com/2018/06/05/apples-core-ml-2-vs-googles-ml-kit-whats-the-difference/)\n- [iOS\uc5d0\uc11c \uba38\uc2e0\ub7ec\ub2dd \uc2ac\ub77c\uc774\ub4dc \uc790\ub8cc](https://docs.google.com/presentation/d/1wA_PAjllpLLcFPuZcERYbQlPe1Ipb-bzIZinZg3zXkg/edit?usp=sharing)\n- [MoT Labs Blog](https://motlabs.github.io/)\n\n### WWDC\n\n#### Core ML\n\n- WWDC2020\n  - [WWDC2020 10152 Session - Use model deployment and security with Core ML](https://developer.apple.com/videos/play/wwdc2020/10152/)\n  - [WWDC2020 10153 Session - Get models on device using Core ML Converters](https://developer.apple.com/videos/play/wwdc2020/10153/)\n  - Vision\n    - [WWDC2020 10673 Session - Explore Computer Vision APIs](https://developer.apple.com/videos/play/wwdc2020/10673/)\n    - [WWDC2020 10099 Session - Explore the Action & Vision app](https://developer.apple.com/videos/play/wwdc2020/10099/)\n    - [WWDC2020 10653 Session - Detect Body and Hand Pose with Vision](https://developer.apple.com/videos/play/wwdc2020/10653/)\n    - [TECH-TALKS 206 Session - QR Code Recognition on iOS 11](https://developer.apple.com/videos/play/tech-talks/206/)\n  - NLP\n    - [WWDC2020 10657 Session - Make apps smarter with Natural Language](https://developer.apple.com/videos/play/wwdc2020/10657/)\n\n- WWDC2019\n  - [WWDC2019 256 Session - Advances in Speech Recognition](https://developer.apple.com/videos/play/wwdc2019/256/)\n  - [WWDC2019 704 Session - Core ML 3 Framework](https://developer.apple.com/videos/play/wwdc2019/704/)\n  - [WWDC2019 228 Session - Creating Great Apps Using Core ML and ARKit](https://developer.apple.com/videos/play/wwdc2019/228/)\n  - [WWDC2019 232 Session - Advances in Natural Language Framework](https://developer.apple.com/videos/play/wwdc2019/232/)\n  - [WWDC2019 222 Session - Understanding Images in Vision Framework](https://developer.apple.com/videos/play/wwdc2019/222/)\n  - [WWDC2019 234 Session - Text Recognition in Vision Framework](https://developer.apple.com/videos/play/wwdc2019/234/)\n- WWDC2018\n  - [WWDC2018 708 Session - What\u2019s New in Core ML, Part 1](https://developer.apple.com/videos/play/wwdc2018/708/)\n  - [WWDC2018 716 Session - Object Tracking in Vision](https://developer.apple.com/videos/play/wwdc2018/716/)\n  - [WWDC2018 717 Session - Vision with Core ML](https://developer.apple.com/videos/play/wwdc2018/717/)\n  - [WWDC2018 709 Session - What\u2019s New in Core ML, Part 2](https://developer.apple.com/videos/play/wwdc2018/709/)\n  - [WWDC2018 713 Session - Introducing Natural Language Framework](https://developer.apple.com/videos/play/wwdc2018/713/)\n- WWDC2017\n  - [WWDC2017 710 Session - Core ML in depth](https://developer.apple.com/videos/play/wwdc2017/710/)\n  - [WWDC2017 208 Session - Natural Language Processing and your Apps](https://developer.apple.com/videos/play/wwdc2017/208/)\n  - [WWDC2017 510 Session - Advances in Core Image: Filters, Metal, Vision, and More](https://developer.apple.com/videos/play/wwdc2017/510/)\n  - [WWDC2017 506 Session - Vision Framework: Building on Core ML](https://developer.apple.com/videos/play/wwdc2017/506/)\n  - [WWDC2017 703 Session - Introducing Core ML](https://developer.apple.com/videos/play/wwdc2017/703/)\n\n#### Create ML and Turi Create\n\n- WWDC2020\n  - [WWDC2020 10642 Session - Build Image and Video Style Transfer models in Create ML](https://developer.apple.com/videos/play/wwdc2020/10642/)\n  - [WWDC2020 10156 Session - Control training in Create ML with Swift](https://developer.apple.com/videos/play/wwdc2020/10156/)\n  - [WWDC2020 10043 Session - Build an Action Classifier with Create ML](https://developer.apple.com/videos/play/wwdc2020/10043/)\n- WWDC2019\n  - [WWDC2019 424 Session - Training Object Detection Models in Create ML](https://developer.apple.com/videos/play/wwdc2019/424/)\n  - [WWDC2019 426 Session - Building Activity Classification Models in Create ML](https://developer.apple.com/videos/play/wwdc2019/426/)\n  - [WWDC2019 420 Session - Drawing Classification and One-Shot Object Detection in Turi Create](https://developer.apple.com/videos/play/wwdc2019/420/)\n  - [WWDC2019 425 Session - Training Sound Classification Models in Create ML](https://developer.apple.com/videos/play/wwdc2019/425/)\n  - [WWDC2019 428 Session - Training Text Classifiers in Create ML](https://developer.apple.com/videos/play/wwdc2019/428/)\n  - [WWDC2019 427 Session - Training Recommendation Models in Create ML](https://developer.apple.com/videos/play/wwdc2019/427/)\n  - [WWDC2019 430 Session - Introducing the Create ML App](https://developer.apple.com/videos/play/wwdc2019/430/)\n- WWDC2018\n  - [WWDC2018 712 Session - A Guide to Turi Create](https://developer.apple.com/videos/play/wwdc2018/712/)\n  - [WWDC2018 703 Session - Introducing Create ML](https://developer.apple.com/videos/play/wwdc2018/703/)\n\n#### Common ML\n\n- WWDC2020\n  - [WWDC2020 10677 Session - Build customized ML models with the Metal Performance Shaders Graph](https://developer.apple.com/videos/play/wwdc2020/10677/)\n- WWDC2019\n  - [WWDC2019 803 Session - Designing Great ML Experiences](https://developer.apple.com/videos/play/wwdc2019/803/)\n  - [WWDC2019 614 Session - Metal for Machine Learning](https://developer.apple.com/videos/play/wwdc2019/614/)\n  - [WWDC2019 209 Session - What's New in Machine Learning](https://developer.apple.com/videos/play/wwdc2019/209/)\n- WWDC2018\n  - [WWDC2018 609 Session - Metal for Accelerating Machine Learning](https://developer.apple.com/videos/play/wwdc2018/609/)\n- WWDC2016\n  - [WWDC2016 715 Session - Neural Networks and Accelerate](https://developer.apple.com/videos/play/wwdc2016/715/)\n  - [WWDC2016 605 Session - What's New in Metal, Part 2](https://developer.apple.com/videos/play/wwdc2016/605/)  \n\n### Metal\n\n- WWDC2020\n  - [WWDC2020 10632 Session - Optimize Metal Performance for Apple Silicon Macs](https://developer.apple.com/videos/play/wwdc2020/10632/)\n  - [WWDC2020 10603 Session - Optimize Metal apps and games with GPU counters](https://developer.apple.com/videos/play/wwdc2020/10603/)\n  - [TECH-TALKS 606 Session - Metal 2 on A11 - Imageblock Sample Coverage Control](https://developer.apple.com/videos/play/tech-talks/606/)\n  - [TECH-TALKS 603 Session - Metal 2 on A11 - Imageblocks](https://developer.apple.com/videos/play/tech-talks/603/)\n  - [TECH-TALKS 602 Session - Metal 2 on A11 - Overview](https://developer.apple.com/videos/play/tech-talks/602/)\n  - [TECH-TALKS 605 Session - Metal 2 on A11 - Raster Order Groups](https://developer.apple.com/videos/play/tech-talks/605/)\n  - [TECH-TALKS 604 Session - Metal 2 on A11 - Tile Shading](https://developer.apple.com/videos/play/tech-talks/604/)\n  - [TECH-TALKS 608 Session - Metal Enhancements for A13 Bionic](https://developer.apple.com/videos/play/tech-talks/608/)\n  - [WWDC2020 10631 Session - Bring your Metal app to Apple Silicon Macs](https://developer.apple.com/videos/play/wwdc2020/10631/)\n  - [WWDC2020 10197 Session - Broaden your reach with Siri Event Suggestions](https://developer.apple.com/videos/play/wwdc2020/10197/)\n  - [WWDC2020 10615 Session - Build GPU binaries with Metal](https://developer.apple.com/videos/play/wwdc2020/10615/)\n  - [WWDC2020 10021 Session - Build Metal-based Core Image kernels with Xcode](https://developer.apple.com/videos/play/wwdc2020/10021/)\n  - [WWDC2020 10616 Session - Debug GPU-side errors in Metal](https://developer.apple.com/videos/play/wwdc2020/10616/)\n  - [WWDC2020 10012 Session - Discover ray tracing with Metal](https://developer.apple.com/videos/play/wwdc2020/10012/)\n  - [WWDC2020 10013 Session - Get to know Metal function pointers](https://developer.apple.com/videos/play/wwdc2020/10013/)\n  - [WWDC2020 10605 Session - Gain insights into your Metal app with Xcode 12](https://developer.apple.com/videos/play/wwdc2020/10605/)\n  - [WWDC2020 10602 Session - Harness Apple GPUs with Metal](https://developer.apple.com/videos/play/wwdc2020/10602/)\n\n### AR\n\n- WWDC2020\n  - [TECH-TALKS 609 Session - Advanced Scene Understanding in AR](https://developer.apple.com/videos/play/tech-talks/609/)\n  - [TECH-TALKS 601 Session - Face Tracking with ARKit](https://developer.apple.com/videos/play/tech-talks/601/)\n  - [WWDC2020 10611 Session - Explore ARKit 4](https://developer.apple.com/videos/play/wwdc2020/10611/)\n  - [WWDC2020 10604 Session - Shop online with AR Quick Look](https://developer.apple.com/videos/play/wwdc2020/10604/)\n  - [WWDC2020 10601 Session - The artist\u2019s AR toolkit](https://developer.apple.com/videos/play/wwdc2020/10601/)\n  - [WWDC2020 10613 Session - What's new in USD](https://developer.apple.com/videos/play/wwdc2020/10613/)\n\n### Examples\n\n- Training\n  - Keras examples: https://keras.io/examples/\n  - Pytorch examples: https://github.com/pytorch/examples\n- Inference\n  - TFLite examples: https://github.com/tensorflow/examples/tree/master/lite\n  - Pytorch Mobile iOS example: https://github.com/pytorch/ios-demo-app\n  - FritzLabs examples: https://github.com/fritzlabs/fritz-examples\n- Models\n  - TensorFlow & TFLite models: https://tfhub.dev/\n  - Pytorch models: https://pytorch.org/hub/\n  - CoreML official models: https://developer.apple.com/machine-learning/models/\n"
}