{
  "repo_name": "nichtdax_awesome-totally-open-chatgpt",
  "readme_content": "<div align=\"center\">\n    <h1>Awesome Totally Open Chatgpt</h1>\n    <a href=\"https://github.com/sindresorhus/awesome\"><img src=\"https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg\"/></a>\n</div>\n\nChatGPT is GPT-3.5 finetuned with RLHF (Reinforcement Learning with Human Feedback) for human instruction and chat.\n\nAlternatives are projects featuring different instruct finetuned language models for chat. \nProjects are **not** counted if they are:\n- Alternative frontend projects which simply call OpenAI's APIs. \n- Using language models which are not finetuned for human instruction or chat.\n\nTags:\n-   Bare: only source code, no data, no model's weight, no chat system\n-   Standard: yes data, yes model's weight, bare chat via API\n-   Full: full yes data, yes model's weight, fancy chat system including TUI and GUI\n-   Complicated: semi open source, not really open source, based on closed model, etc...\n\nOther revelant lists:\n- [yaodongC/awesome-instruction-dataset](https://github.com/yaodongC/awesome-instruction-dataset): A collection of open-source dataset to train instruction-following LLMs (ChatGPT,LLaMA,Alpaca)\n\n# Table of Contents\n1. [The template](#The-template)\n2. [The list](#The-list)\n   - [lucidrains/PaLM-rlhf-pytorch](#lucidrainsPaLM-rlhf-pytorch)\n   - [togethercomputer/OpenChatKit](#togethercomputerOpenChatKit)\n   - [oobabooga/text-generation-webui](#oobaboogatext-generation-webui)\n   - [KoboldAI/KoboldAI-Client](#KoboldAIKoboldAI-Client)\n   - [LAION-AI/Open-Assistant](#LAION-AIOpen-Assistant)\n   - [tatsu-lab/stanford_alpaca](#tatsu-labstanford_alpaca)\n     - [Other LLaMA-derived projects](#other-llama-derived-projects)\n   - [BlinkDL/ChatRWKV](#BlinkDLChatRWKV)\n   - [THUDM/ChatGLM-6B](#THUDMChatGLM-6B)\n   - [bigscience-workshop/xmtf](#bigscience-workshopxmtf)\n   - [carperai/trlx](#carperaitrlx)\n   - [databrickslabs/dolly](#databrickslabsdolly)\n   - [LianjiaTech/BELLE](#lianjiatechbelle)\n   - [ethanyanjiali/minChatGPT](#ethanyanjialiminchatgpt)\n   - [cerebras/Cerebras-GPT](#cerebrascerebras-gpt)\n   - [TavernAI/TavernAI](#tavernaitavernai)\n   - [Cohee1207/SillyTavern](#cohee1207sillytavern)\n   - [h2oai/h2ogpt](#h2oaih2ogpt)\n   - [mlc-ai/web-llm](#mlc-aiweb-llm)\n   - [Stability-AI/StableLM](#stability-aistablelm)\n   - [clue-ai/ChatYuan](#clue-aichatyuan)\n   - [OpenLMLab/MOSS](#openlmlabmoss)\n\n# The template\n\nAppend the new project at the end of file\n\n```markdown\n## [{owner}/{project-name}]{https://github.com/link/to/project}\n\nDescription goes here\n\nTags: Bare/Standard/Full/Complicated\n```\n\n# The list\n\n## [lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch)\n\nImplementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM\n\nTags: Bare\n\n## [togethercomputer/OpenChatKit](https://github.com/togethercomputer/OpenChatKit)\n\nOpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. \n\nRelated links:\n- [spaces/togethercomputer/OpenChatKit](https://huggingface.co/spaces/togethercomputer/OpenChatKit)\n\nTags: Full\n\n## [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)\n\nA gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, LLaMA, and Pygmalion.\n\nTags: Full\n\n## [KoboldAI/KoboldAI-Client](https://github.com/KoboldAI/KoboldAI-Client)\n\nThis is a browser-based front-end for AI-assisted writing with multiple local & remote AI models. It offers the standard array of tools, including Memory, Author\u2019s Note, World Info, Save & Load, adjustable AI settings, formatting options, and the ability to import existing AI Dungeon adventures. You can also turn on Adventure mode and play the game like AI Dungeon Unleashed.\n\nTags: Full\n\n## [LAION-AI/Open-Assistant](https://github.com/LAION-AI/Open-Assistant) \n\nOpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.\n\nRelated links:\n- [huggingface.co/OpenAssistant](https://huggingface.co/OpenAssistant)\n- [r/OpenAssistant/](https://www.reddit.com/r/OpenAssistant/)\n\nTags: Full\n\n## [tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)\n\nThis is the repo for the Stanford Alpaca project, which aims to build and share an instruction-following LLaMA model.\n\nTags: Complicated\n\n### Other LLaMA-derived projects:\n\n- [pointnetwork/point-alpaca](https://github.com/pointnetwork/point-alpaca) Released weights recreated from Stanford Alpaca, an experiment in fine-tuning LLaMA on a synthetic instruction dataset.\n- [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora) Code for rproducing the Stanford Alpaca results using low-rank adaptation (LoRA).\n- [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) Ports for inferencing LLaMA in C/C++ running on CPUs, supports alpaca, gpt4all, etc.\n- [setzer22/llama-rs](https://github.com/setzer22/llama-rs) Rust port of the llama.cpp project.\n- [juncongmoo/chatllama](https://github.com/juncongmoo/chatllama) Open source implementation for LLaMA-based ChatGPT runnable in a single GPU.\n- [Lightning-AI/lit-llama](https://github.com/Lightning-AI/lit-llama) Implementation of the LLaMA language model based on nanoGPT.\n- [nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all) Demo, data and code to train an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMA.\n- [hpcaitech/ColossalAI#ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat) An open-source solution for cloning ChatGPT with a complete RLHF pipeline.\n- [lm-sys/FastChat](https://github.com/lm-sys/FastChat) An open platform for training, serving, and evaluating large language model based chatbots.\n- [nsarrazin/serge](https://github.com/nsarrazin/serge) A web interface for chatting with Alpaca through llama.cpp. Fully dockerized, with an easy to use API.\n\n## [BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV)\n\nChatRWKV is like ChatGPT but powered by RWKV (100% RNN) language model, and open source.\n\nTags: Full\n\n## [THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)\n\nChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level).\n\nRelated links:\n\n- Alternative Web UI: [Akegarasu/ChatGLM-webui](https://github.com/Akegarasu/ChatGLM-webui)\n- Slim version (remove 20K image tokens to reduce memory usage): [silver/chatglm-6b-slim](https://huggingface.co/silver/chatglm-6b-slim)\n- Fintune ChatGLM-6b using low-rank adaptation (LoRA): [lich99/ChatGLM-finetune-LoRA](https://github.com/lich99/ChatGLM-finetune-LoRA)\n- Deploying ChatGLM on Modelz: [tensorchord/modelz-ChatGLM](https://github.com/tensorchord/modelz-ChatGLM)\n- Docker image with built-on playground UI and streaming API compatible with OpenAI, using [Basaran](https://github.com/hyperonym/basaran): [peakji92/chatglm:6b](https://hub.docker.com/r/peakji92/chatglm/tags)\n\nTags: Full\n\n## [bigscience-workshop/xmtf](https://github.com/bigscience-workshop/xmtf)\n\nThis repository provides an overview of all components used for the creation of BLOOMZ & mT0 and xP3 introduced in the paper [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/abs/2211.01786).\n\nRelated links:\n- [bigscience/bloomz](https://huggingface.co/bigscience/bloomz)\n- [bigscience/mt0-base](https://huggingface.co/bigscience/mt0-base)\n\nTags: Standard\n\n## [carperai/trlx](https://github.com/carperai/trlx)\n\n A repo for distributed training of language models with Reinforcement Learning via Human Feedback (RLHF), supporting online RL up to 20b params and offline RL to larger models. Basically what you would use to finetune GPT into ChatGPT. \n\nTags: Bare\n\n## [databrickslabs/dolly](https://github.com/databrickslabs/dolly)\n\nDatabricks\u2019 dolly-v2-12b, an instruction-following large language model trained on the Databricks machine learning platform that is licensed for commercial use. Based on pythia-12b trained on ~15k instruction/response fine tuning records [databricks-dolly-15k](https://github.com/databrickslabs/dolly/tree/master/data) generated by Databricks employees in capability domains from the InstructGPT paper.\n\nRelated links:\n- [dolly v2 12B commercial commercially available model](https://huggingface.co/databricks/dolly-v2-12b)\n- [dolly v1 6b model card](https://huggingface.co/databricks/dolly-v1-6b)\n\nTags: Standard\n\n## [LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE)\n\nThe goal of this project is to promote the development of the open-source community for Chinese language large-scale conversational models. This project optimizes Chinese performance in addition to original Stanford Alpaca. The model finetuning uses only data generated via ChatGPT (without other data). This repo contains: 175 chinese seed tasks used for generating the data, code for generating the data, 0.5M generated data used for fine-tuning the model, model finetuned from BLOOMZ-7B1-mt on data generated by this project.\n\nRelated links:\n- [English readme](https://github.com/LianjiaTech/BELLE#-belle-be-large-language-model-engine-1)\n\nTags: Standard\n\n## [ethanyanjiali/minChatGPT](https://github.com/ethanyanjiali/minChatGPT)\n\nA minimum example of aligning language models with RLHF similar to ChatGPT\n\nRelated links:\n- [huggingface.co/ethanyanjiali/minChatGPT](https://huggingface.co/ethanyanjiali/minChatGPT)\n\nTags: Standard\n\n## [cerebras/Cerebras-GPT](https://huggingface.co/cerebras/Cerebras-GPT-6.7B)\n\n7 open source GPT-3 style models with parameter ranges from 111 million to 13 billion, trained using the [Chinchilla](https://arxiv.org/abs/2203.15556) formula. Model weights have been released under a permissive license (Apache 2.0 license in particular).\n\nRelated links:\n- [Announcement](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/)\n- [Models with other amount of parameters](https://huggingface.co/cerebras)\n\nTags: Standard\n\n## [TavernAI/TavernAI](https://github.com/TavernAI/TavernAI)\n\nAtmospheric adventure chat for AI language model **Pygmalion** by default and other models such as **KoboldAI**, ChatGPT, GPT-4\n\nTags: Full\n\n## [Cohee1207/SillyTavern](https://github.com/Cohee1207/SillyTavern)\n\nSillyTavern is a fork of TavernAI 1.2.8 which is under more active development, and has added many major features. At this point they can be thought of as completely independent programs. On its own Tavern is useless, as it's just a user interface. You have to have access to an AI system backend that can act as the roleplay character. There are various supported backends: OpenAPI API (GPT), KoboldAI (either running locally or on Google Colab), and more.\n\nTags: Full\n\n## [h2oai/h2ogpt](https://github.com/h2oai/h2ogpt)\n\nh2oGPT - The world's best open source GPT\n- Open-source repository with fully permissive, commercially usable code, data and models\n- Code for preparing large open-source datasets as instruction datasets for fine-tuning of large language models (LLMs), including prompt engineering\n- Code for fine-tuning large language models (currently up to 20B parameters) on commodity hardware and enterprise GPU servers (single or multi node)\n- Code to run a chatbot on a GPU server, with shareable end-point with Python client API\n- Code to evaluate and compare the performance of fine-tuned LLMs\n\nRelated links:\n- [h2oGPT 20B](https://gpt.h2o.ai/)\n- [\ud83e\udd17 h2oGPT 12B #1](https://huggingface.co/spaces/h2oai/h2ogpt-chatbot)\n- [\ud83e\udd17 h2oGPT 12B #2](https://huggingface.co/spaces/h2oai/h2ogpt-chatbot2)\n\nTags: Full\n\n## [mlc-ai/web-llm](https://github.com/mlc-ai/web-llm)\n\nBringing large-language models and chat to web browsers. Everything runs inside the browser with no server support.\n\nRelated links:\n- https://mlc.ai/web-llm\n\nTags: Full\n\n## [Stability-AI/StableLM](https://github.com/Stability-AI/StableLM)\n\nThis repository contains Stability AI's ongoing development of the StableLM series of language models and will be continuously updated with new checkpoints.\n\nRelated links:\n- [huggingface.co/spaces/stabilityai/stablelm-tuned-alpha-chat](https://huggingface.co/spaces/stabilityai/stablelm-tuned-alpha-chat)\n- [StableVicuna](https://github.com/Stability-AI/StableLM#stablevicuna) an RLHF fine-tune of Vicuna-13B v0, which itself is a fine-tune of LLaMA-13B. \n\nTags: Full\n\n## [clue-ai/ChatYuan](https://github.com/clue-ai/ChatYuan)\n\nChatYuan: Large Language Model for Dialogue in Chinese and English (The repos are mostly in Chinese)\n\nRelated links:\n- [A bit translated readme to English](https://github.com/nichtdax/awesome-totally-open-chatgpt/issues/18#issuecomment-1492826662)\n\nTags: Full\n\n\n## [OpenLMLab/MOSS](https://github.com/OpenLMLab/MOSS)\n\nMOSS: An open-source tool-augmented conversational language model from Fudan University. (Most examples are in Chinese)\n\nRelated links:\n- [English readme](https://github.com/OpenLMLab/MOSS/blob/main/README_en.md)\n\nTags: Full\n\n"
}